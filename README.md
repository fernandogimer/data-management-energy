# Data Management â€” PredicciÃ³n de Demanda EnergÃ©tica (Barcelona)

Repositorio del proyecto de la asignatura Data Management.

**Objetivo:** Predecir y explicar la demanda energÃ©tica en Barcelona integrando datos estructurados (BigQuery) y no estructurados / ML (Azure). VisualizaciÃ³n en Power BI/Looker Studio.

---

## ğŸ¯ Objetivos

- Integrar CSV pÃºblicos de consumo (CP / sector / tramo horario, 2022â€“2025) en una tabla Ãºnica particionada en BigQuery
- Enriquecer con contexto (meteo, festivos y eventos/alertas) para explicar picos
- Modelar (baseline + AutoML/ML en Azure) para mejorar la precisiÃ³n frente a un baseline histÃ³rico
- Medir y comunicar mediante KPIs (ej.: XX) y dashboard

---

## ğŸ§© Alcance (MVP)

**Estructurados (BigQuery):** Unificar 2022â€“2025 en `Data_Management.consumo_barcelona` 
- **Partition by:** `fecha`
- **Cluster by:** `codigo_postal`, `sector_economico`

**Vistas BI/ML:**
- Generar vista

**Checks:** Nulos, negativos, duplicados y rangos

**Dashboard mÃ­nimo:** Serie real vs. (luego) pred, KPIs bÃ¡sicos

**IteraciÃ³n siguiente:** IngestiÃ³n meteo/festivos/eventos + entrenamiento/AutoML en Azure

---

## ğŸ—ï¸ Arquitectura (alto nivel)

### Flujo por capas

```
[CSV consumo BCN 2022â€“2025] 
    â†“
Google BigQuery (dataset: Data_Management)
    â†“
(1) Tabla Ãºnica: consumo_barcelona
    â€¢ Partition By: fecha (DATE)
    â€¢ Cluster By: codigo_postal, sector_economico
    â†“
(2) Vistas:
    â€¢ vw_consumo_bcn_current (filtro calidad BI/ML)
    â†“
(3) BI: Power BI / Looker Studio
    â†“
(4) Enriquecimiento (prÃ³xima fase):
    â€¢ Meteo (AEMET/Meteocat)
    â€¢ Festivos (ICS)
    â€¢ Eventos/alertas (Azure Logic Apps + Azure AI Text Analytics)
    â€¢ Joins en BigQuery
    â€¢ Entrenamiento/AutoML en Azure
    â€¢ Persistencia de predicciones
```

### Principios clave

- **Particionado por fecha** para coste/velocidad
- **Vistas estables** para no romper BI/ML al cambiar tablas base
- **Trazabilidad:** `anio_origen`, `load_ts`

---

## ğŸ—‚ï¸ Estructura del repositorio

```
bq/
â”œâ”€â”€ ddl/
â”‚   â””â”€â”€ 01_create_consumo_barcelona.sql   # CTAS que unifica 2022â€“2025 en tabla Ãºnica
â”œâ”€â”€ views/
â”‚   â”œâ”€â”€ vw_consumo_bcn_current.sql        # Vista con filtro bÃ¡sico de calidad
â”‚   â””â”€â”€ vw_consumo_bcn_con_horas.sql      # (Opcional) Parseo de horas desde tramo_horario
â””â”€â”€ checks/
    â””â”€â”€ checks_basicos.sql                # Nulos/negativos/duplicados + rangos

docs/
â””â”€â”€ decisiones.md                         # Registro de cambios/decisiones (quÃ©/por quÃ©/cuÃ¡ndo/quiÃ©n)

README.md
.gitignore
```


---

## ğŸ§± Esquema (MVP)

### Tabla: `Data_Management.consumo_barcelona`

**Partition By:** `fecha` (DATE) Â· **Cluster By:** `cp5`, `sector_economico`

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `fecha` | DATE | Fecha del registro |
| `codigo_postal` | STRING | CÃ³digo postal (5 dÃ­gitos) |
| `sector_economico` | STRING | Sector econÃ³mico |
| `tramo_horario` | STRING | "De 00:00:00 a 06:00:00" / "No consta" |
| `kwh` | FLOAT64 | Consumo del tramo en kWh |
| `anio_origen` | STRING | AÃ±o/origen del registro (p. ej., "2023") |
| `load_ts` | TIMESTAMP | Timestamp de carga/transformaciÃ³n |

### Vistas sugeridas

- **`vw_consumo_bcn_current`:** Excluye "No consta" y kWh nulos/negativos
- **`vw_consumo_bcn_con_horas`:** Extrae `hora_ini`/`hora_fin`

---

## ğŸ“¥ CÃ³mo reproducir (paso a paso)

### 1. Crear/actualizar la tabla Ãºnica (CTAS)

Ejecuta en BigQuery: 

```bash
bq/ddl/01_create_consumo_barcelona.sql
```

### 2. Conectar BI

Conecta Power BI / Looker Studio a `vw_consumo_bcn_current`

---

## âœ… Calidad de datos (mÃ­nimo viable)

- **Completitud:** `kwh` no nulo â‰¥ 99%
- **Validez:** `kwh` â‰¥ 0; `fecha` en rango dataset
- **Unicidad lÃ³gica:** (`fecha`, `codigoo_postal`, `tramo_horario`, `sector_economico`) sin duplicados
- **Trazabilidad:** `anio_origen`, `load_ts`

---

## ğŸ“Š KPIs

aplicar kpis que seleccionamos


---

## ğŸ“ Licencia

_[Especifica tu licencia aquÃ­]_

## ğŸ‘¤ Autores

_[AÃ±ade los nombres del equipo aquÃ­]_
---

## Subproyecto: Pipeline de Ingesta y Enriquecimiento NLP con Databricks

Como parte del proceso de ingesta de datos, se desarrollÃ³ un pipeline de datos independiente en **Azure Databricks** para generar y enriquecer una de las fuentes de datos.

- **Objetivo:** Demostrar la orquestaciÃ³n de un pipeline end-to-end en Databricks, incluyendo la simulaciÃ³n de datos, el enriquecimiento con una API de IA Generativa (Google Gemini) y la exportaciÃ³n de resultados a BigQuery.
- **TecnologÃ­as:** Azure Databricks, Python (PySpark, Pandas), Google Gemini API.
- **Resultado:** La tabla `bronze_data.precios_actividad_nlp_manual` en BigQuery.
- **Detalles:** Ver el [informe tÃ©cnico completo](docs/informe_pipeline_nlp.md) y el [cÃ³digo del notebook](notebooks/databricks/pipeline_databricks_nlp.py).







